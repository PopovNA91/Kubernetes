Приложение запущено и работает, но время от времени появляется необходимость передавать между бекендами данные. А сам бекенд генерирует статику для фронта. Нужно оптимизировать это. Для настройки NFS сервера можно воспользоваться следующей инструкцией (производить под пользователем на сервере, у которого есть доступ до kubectl):  
*    установить helm: curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash  
*    добавить репозиторий чартов: helm repo add stable https://charts.helm.sh/stable && helm repo update  
*    установить nfs-server через helm: helm install nfs-server stable/nfs-server-provisioner  

В конце установки будет выдан пример создания PVC для этого сервера. 
```
root@cp:/home/nikolay/stage# curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 11156  100 11156    0     0  30315      0 --:--:-- --:--:-- --:--:-- 30315
Helm v3.8.1 is available. Changing from version v3.8.0.
Downloading https://get.helm.sh/helm-v3.8.1-linux-amd64.tar.gz
Verifying checksum... Done.
Preparing to install helm into /usr/local/bin
helm installed into /usr/local/bin/helm
```
```
root@cp:/home/nikolay/stage# helm repo add stable https://charts.helm.sh/stable && helm repo update
"stable" has been added to your repositories
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "stable" chart repository
Update Complete. ⎈Happy Helming!⎈
```
```
root@cp:/home/nikolay/stage# helm install nfs-server stable/nfs-server-provisioner
WARNING: This chart is deprecated
NAME: nfs-server
LAST DEPLOYED: Sun Apr  3 20:59:31 2022
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
The NFS Provisioner service has now been installed.

A storage class named 'nfs' has now been created
and is available to provision dynamic volumes.

You can use this storageclass by creating a `PersistentVolumeClaim` with the
correct storageClassName attribute. For example:

    ---
    kind: PersistentVolumeClaim
    apiVersion: v1
    metadata:
      name: test-dynamic-volume-claim
    spec:
      storageClassName: "nfs"
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 100Mi

```
___
## Задание 1: подключить для тестового конфига общую папку  

В stage окружении часто возникает необходимость отдавать статику бекенда сразу фронтом. Проще всего сделать это через общую папку. Требования:  
*    в поде подключена общая папка между контейнерами (например, /static);
*    после записи чего-либо в контейнере с беком файлы можно получить из контейнера с фронтом.
___
Использовал пример из ![документации](https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/)
Подготовил ![Манифесты]()
```
root@cp:/home/nikolay/stage# kubectl apply -f .
namespace/stage unchanged
deployment.apps/two-containers created
```
```
root@two-containers:/# apt-get update
root@two-containers:/# apt-get install curl procps
```
```
root@two-containers:/# ps aux
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root           1  0.0  0.2   8848  5448 ?        Ss   20:56   0:00 nginx: master process nginx -g daemon
nginx         31  0.0  0.1   9236  2540 ?        S    20:56   0:00 nginx: worker process
nginx         32  0.0  0.1   9236  2540 ?        S    20:56   0:00 nginx: worker process
root          45  0.0  0.1   4092  3476 pts/0    Ss   21:13   0:00 /bin/bash
root         390  0.0  0.1   6692  2980 pts/0    R+   21:15   0:00 ps aux
root@two-containers:/# curl localhost
Hello from the debian container
```

## Задание 2: подключить общую папку для прода

Поработав на stage, доработки нужно отправить на прод. В продуктиве у нас контейнеры крутятся в разных подах, поэтому потребуется PV и связь через PVC. Сам PV должен быть связан с NFS сервером. Требования:
*    все бекенды подключаются к одному PV в режиме ReadWriteMany;
*    фронтенды тоже подключаются к этому же PV с таким же режимом;
*    файлы, созданные бекендом, должны быть доступны фронту.
___


```
root@cp:/home/nikolay/prod# kubectl get pod,deployment,sts,svc,ep -n prod
NAME                            READY   STATUS    RESTARTS   AGE
pod/backend-6f698bd866-d7lbr    0/1     Pending   0          4m47s
pod/frontend-86c955b8df-mxpts   0/1     Pending   0          3m52s
pod/postgres-statefulset-0      1/1     Running   0          49s

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/backend    0/1     1            0           4m47s
deployment.apps/frontend   0/1     1            0           3m52s

NAME                                    READY   AGE
statefulset.apps/postgres-statefulset   1/1     49s

NAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/backend-svc        ClusterIP   10.233.63.87    <none>        9000/TCP   4m47s
service/frontend-svc       ClusterIP   10.233.55.186   <none>        8000/TCP   3m52s
service/postgres-service   ClusterIP   10.233.62.27    <none>        5432/TCP   32s

NAME                         ENDPOINTS   AGE
endpoints/backend-svc        <none>      4m47s
endpoints/frontend-svc       <none>      3m52s
endpoints/postgres-service   <none>      32s
```

```
root@cp:/home/nikolay/prod# kubectl get pod,deployment -n prod
NAME                            READY   STATUS              RESTARTS   AGE
pod/backend-6f698bd866-d7lbr    0/1     ContainerCreating   0          6m27s
pod/frontend-86c955b8df-mxpts   0/1     ContainerCreating   0          5m32s
pod/postgres-statefulset-0      1/1     Running             0          2m29s

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/backend    0/1     1            0           6m27s
deployment.apps/frontend   0/1     1            0           5m32s
```

```
root@cp:/home/nikolay/prod# kubectl describe pod backend-6f698bd866-d7lbr -n prod
Name:           backend-6f698bd866-d7lbr
Namespace:      prod
Priority:       0
Node:           node1/10.131.0.5
Start Time:     Mon, 04 Apr 2022 19:23:38 +0000
Labels:         app=backend
                pod-template-hash=6f698bd866
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/backend-6f698bd866
Containers:
  backend:
    Container ID:   
    Image:          popovna/app-backend
    Image ID:       
    Port:           9000/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Environment:
      DATABASE_URL:  postgres://postgres:postgres@postgres:5432/news
    Mounts:
      /static from share (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wrq7n (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  share:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  share-volume-claim
    ReadOnly:   false
  kube-api-access-wrq7n:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                   From               Message
  ----     ------            ----                  ----               -------
  Warning  FailedScheduling  34m                   default-scheduler  0/3 nodes are available: 3 persistentvolumeclaim "share-volume-claim" not found.
  Warning  FailedScheduling  28m (x5 over 33m)     default-scheduler  0/3 nodes are available: 3 persistentvolumeclaim "share-volume-claim" not found.
  Normal   Scheduled         28m                   default-scheduler  Successfully assigned prod/backend-6f698bd866-d7lbr to node1
  Warning  FailedMount       5m46s (x10 over 26m)  kubelet            Unable to attach or mount volumes: unmounted volumes=[share], unattached volumes=[share kube-api-access-wrq7n]: timed out waiting for the condition
  Warning  FailedMount       107s (x21 over 28m)   kubelet            MountVolume.SetUp failed for volume "pvc-c3eef43b-14e1-4ad0-b79b-86e720db323c" : mount failed: exit status 32
Mounting command: mount
Mounting arguments: -t nfs -o vers=3 10.233.57.32:/export/pvc-c3eef43b-14e1-4ad0-b79b-86e720db323c /var/lib/kubelet/pods/c35732b9-9dd7-46a8-8c59-63426776bb3c/volumes/kubernetes.io~nfs/pvc-c3eef43b-14e1-4ad0-b79b-86e720db323c
Output: mount: /var/lib/kubelet/pods/c35732b9-9dd7-46a8-8c59-63426776bb3c/volumes/kubernetes.io~nfs/pvc-c3eef43b-14e1-4ad0-b79b-86e720db323c: bad option; for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
```
```
# Решение проблемы (необходимо установить на всех нодах !!!):
sudo apt install nfs-common
```

```
root@cp:/home/nikolay/prod# kubectl get pod,deployment -n prod
NAME                            READY   STATUS    RESTARTS   AGE
pod/backend-6f698bd866-d7lbr    1/1     Running   0          44m
pod/frontend-86c955b8df-mxpts   1/1     Running   0          43m
pod/postgres-statefulset-0      1/1     Running   0          40m

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/backend    1/1     1            1           44m
deployment.apps/frontend   1/1     1            1           43m
````

```
root@cp:/home/nikolay/prod# kubectl exec frontend-86c955b8df-mxpts -n prod -c frontend-prod -- ls -la /static
total 8
drwxrwsrwx 2 root root 4096 Apr  4 19:23 .
drwxr-xr-x 1 root root 4096 Apr  4 19:58 ..
root@cp:/home/nikolay/prod# kubectl exec frontend-86c955b8df-mxpts -n prod -c frontend-prod -- sh -c 'echo "test" > /static/test.txt'
root@cp:/home/nikolay/prod# kubectl exec frontend-86c955b8df-mxpts -n prod -c frontend-prod -- ls -la /static
total 12
drwxrwsrwx 2 root root 4096 Apr  4 20:18 .
drwxr-xr-x 1 root root 4096 Apr  4 19:58 ..
-rw-r--r-- 1 root root    5 Apr  4 20:18 test.txt
root@cp:/home/nikolay/prod# kubectl exec backend-6f698bd866-d7lbr -n prod -c backend -- ls -la /static
total 12
drwxrwsrwx 2 root root 4096 Apr  4 20:18 .
drwxr-xr-x 1 root root 4096 Apr  4 19:58 ..
-rw-r--r-- 1 root root    5 Apr  4 20:18 test.txt
root@cp:/home/nikolay/prod# kubectl exec backend-6f698bd866-d7lbr -n prod -c backend -- cat /static/test.txt
test
```
