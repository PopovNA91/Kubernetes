## Задание 1: проверить работоспособность каждого компонента
Для проверки работы можно использовать 2 способа: port-forward и exec. Используя оба способа, проверьте каждый компонент:
*    сделайте запросы к бекенду;
*    сделайте запросы к фронту;
*    подключитесь к базе данных.
___

```
root@cp:/home/nikolay/prod# kubectl exec frontend-86c955b8df-bk4qs -n prod -c frontend-prod -- curl localhost:80
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0<!DOCTYPE html>
<html lang="ru">
<head>
    <title>Список</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/build/main.css" rel="stylesheet">
</head>
<body>
    <main class="b-page">
        <h1 class="b-page__title">Список</h1>
        <div class="b-page__content b-items js-list"></div>
    </main>
    <script src="/build/main.js"></script>
</body>
100   448  100   448    0     0   437k      0 --:--:-- --:--:-- --:--:--  437k
```

## Задание 2: ручное масштабирование
При работе с приложением иногда может потребоваться вручную добавить пару копий. Используя команду kubectl scale, попробуйте увеличить 
количество бекенда и фронта до 3. Проверьте, на каких нодах оказались копии после каждого действия (kubectl describe, kubectl get pods -o wide). 
После уменьшите количество копий до 1.
___

```
root@cp:/home/nikolay/prod# kubectl scale --replicas=3 deployment.apps/backend -n prod
deployment.apps/backend scaled
root@cp:/home/nikolay/prod# kubectl scale --replicas=3 deployment.apps/frontend -n prod
deployment.apps/frontend scaled
```

```
root@cp:/home/nikolay/prod# kubectl get nodes
NAME          STATUS   ROLES                  AGE   VERSION
cp            Ready    control-plane,master   45m   v1.23.4
kub-ingress   Ready    <none>                 43m   v1.23.4
node1         Ready    <none>                 43m   v1.23.4
root@cp:/home/nikolay/prod# kubectl get pod,deployment -n prod
NAME                            READY   STATUS    RESTARTS   AGE
pod/backend-6f698bd866-7hls2    1/1     Running   0          25m
pod/backend-6f698bd866-mjjs8    1/1     Running   0          3m35s
pod/backend-6f698bd866-rgznm    1/1     Running   0          3m35s
pod/frontend-86c955b8df-4ff4j   1/1     Running   0          3m17s
pod/frontend-86c955b8df-bk4qs   1/1     Running   0          24m
pod/frontend-86c955b8df-gdns9   1/1     Running   0          3m17s
pod/postgres-statefulset-0      1/1     Running   0          26m

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/backend    3/3     3            3           25m
deployment.apps/frontend   3/3     3            3           24m
root@cp:/home/nikolay/prod# kubectl get pods -o wide -n prod
NAME                        READY   STATUS    RESTARTS   AGE     IP            NODE          NOMINATED NODE   READINESS GATES
backend-6f698bd866-7hls2    1/1     Running   0          26m     10.233.90.3   node1         <none>           <none>
backend-6f698bd866-mjjs8    1/1     Running   0          4m5s    10.233.90.4   node1         <none>           <none>
backend-6f698bd866-rgznm    1/1     Running   0          4m5s    10.233.79.4   kub-ingress   <none>           <none>
frontend-86c955b8df-4ff4j   1/1     Running   0          3m47s   10.233.90.5   node1         <none>           <none>
frontend-86c955b8df-bk4qs   1/1     Running   0          25m     10.233.79.3   kub-ingress   <none>           <none>
frontend-86c955b8df-gdns9   1/1     Running   0          3m47s   10.233.79.5   kub-ingress   <none>           <none>
postgres-statefulset-0      1/1     Running   0          26m     10.233.79.2   kub-ingress   <none>           <none>
```
```
root@cp:/home/nikolay/prod# kubectl describe deployment.apps/backend -n prod
Name:                   backend
Namespace:              prod
CreationTimestamp:      Wed, 06 Apr 2022 19:52:47 +0000
Labels:                 app=backend
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=backend
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=backend
  Containers:
   backend:
    Image:      popovna/app-backend
    Port:       9000/TCP
    Host Port:  0/TCP
    Environment:
      DATABASE_URL:  postgres://postgres:postgres@postgres:5432/news
    Mounts:
      /static from share (rw)
  Volumes:
   share:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  share-volume-claim
    ReadOnly:   false
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   backend-6f698bd866 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  31m   deployment-controller  Scaled up replica set backend-6f698bd866 to 1
  Normal  ScalingReplicaSet  9m5s  deployment-controller  Scaled up replica set backend-6f698bd866 to 3
```

```
root@cp:/home/nikolay/prod# kubectl describe deployment.apps/frontend -n prod
Name:                   frontend
Namespace:              prod
CreationTimestamp:      Wed, 06 Apr 2022 19:54:09 +0000
Labels:                 app=frontend
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=frontend
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=frontend
  Containers:
   frontend-prod:
    Image:      popovna/app-frontend
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:
      BASE_URL:  http://localhost:9000
    Mounts:
      /static from share (rw)
  Volumes:
   share:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  share-volume-claim
    ReadOnly:   false
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   frontend-86c955b8df (3/3 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  31m    deployment-controller  Scaled up replica set frontend-86c955b8df to 1
  Normal  ScalingReplicaSet  9m49s  deployment-controller  Scaled up replica set frontend-86c955b8df to 3
```
```
root@cp:/home/nikolay/prod# kubectl scale --replicas=1 deployment.apps/frontend -n prod
deployment.apps/frontend scaled
root@cp:/home/nikolay/prod# kubectl scale --replicas=1 deployment.apps/backend -n prod
deployment.apps/backend scaled
```
root@cp:/home/nikolay/prod# kubectl get pods -o wide -n prod
NAME                        READY   STATUS    RESTARTS   AGE   IP            NODE          NOMINATED NODE   READINESS GATES
backend-6f698bd866-rgznm    1/1     Running   0          11m   10.233.79.4   kub-ingress   <none>           <none>
frontend-86c955b8df-4ff4j   1/1     Running   0          11m   10.233.90.5   node1         <none>           <none>
postgres-statefulset-0      1/1     Running   0          34m   10.233.79.2   kub-ingress   <none>           <none>
```
 
